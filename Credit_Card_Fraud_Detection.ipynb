{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNi6YqixTuoz"
      },
      "outputs": [],
      "source": [
        "\n",
        "import kagglehub\n",
        "kartik2112_fraud_detection_path = kagglehub.dataset_download('kartik2112/fraud-detection')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ksm333STuo2"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqRfRSH-Tuo3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns',None)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(18,8)},style='darkgrid')\n",
        "sns.set_palette('rocket')\n",
        "from time import time\n",
        "import pingouin\n",
        "from scipy.stats import ttest_ind\n",
        "from datasist.structdata import detect_outliers\n",
        "from geopy.distance import great_circle\n",
        "from category_encoders import WOEEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import *\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDvPIfY5Tuo3"
      },
      "source": [
        "# Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU5s3mnGTuo3"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(r\"E:\\Data Science\\Datasets\\Credit Card Fraud Detection\\fraudTrain.csv\")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RuJfjPqTuo3"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(r\"E:\\Data Science\\Datasets\\Credit Card Fraud Detection\\fraudTest.csv\")\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEBkrCRVTuo4"
      },
      "outputs": [],
      "source": [
        "train['split'] = 'train'\n",
        "test['split']='test'\n",
        "df = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXfN1iufTuo4"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ng3MM4fTuo4"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O27WQigKTuo5"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLyNs7i-Tuo5"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQmO7bb-Tuo5"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82J6QMjPTuo5"
      },
      "outputs": [],
      "source": [
        "\n",
        "df.drop(columns=['Unnamed: 0','street','state','first','last','trans_num','unix_time'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA3QGH1gTuo5"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeLJyFt5Tuo5"
      },
      "outputs": [],
      "source": [
        "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'],format='mixed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVxN1PrUTuo5"
      },
      "outputs": [],
      "source": [
        "df['hour'] = df['trans_date_trans_time'].dt.hour\n",
        "df['day'] = df['trans_date_trans_time'].dt.day_name()\n",
        "df['month'] = df['trans_date_trans_time'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD0ZlaKeTuo5"
      },
      "outputs": [],
      "source": [
        "df['merchant'] = df['merchant'].apply(lambda x : x.replace('fraud_',''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBcvoOIXTuo5"
      },
      "outputs": [],
      "source": [
        "df[['merchant']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KleyZOVYTuo5"
      },
      "outputs": [],
      "source": [
        "df['dob'] = pd.to_datetime(df['dob'],format='mixed')\n",
        "df['age'] = (df['trans_date_trans_time'].dt.year - df['dob'].dt.year).astype(int)\n",
        "df.drop(columns='dob',inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaqrfjPkTuo6"
      },
      "outputs": [],
      "source": [
        "df['distance_km'] = df.apply(lambda col : round(great_circle((col['lat'],col['long']),\n",
        "                                         (col['merch_lat'],col['merch_long'])).kilometers,2),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1-VKcO2Tuo6"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['lat','long','merch_lat','merch_long'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1Z4N0GKTuo6"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ9ZQVtqTuo6"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n9WwkuOTuo6"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlIpZxeJTuo6"
      },
      "outputs": [],
      "source": [
        "df.describe(include='object').T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57trfaD4Tuo6"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(df.select_dtypes(include='number').corr(),\n",
        "            annot=None,cmap='coolwarm',fmt='.2f',linewidths=0.5,cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ7iSXjyTuo6"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include='number').corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1IHJJMrTuo6"
      },
      "source": [
        "##### Get in depth in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wVUpAK7Tuo6"
      },
      "outputs": [],
      "source": [
        "def bar_plot(col):\n",
        "    def top_frauds(col):\n",
        "        return pd.DataFrame(df.loc[df['is_fraud']==1,[col]].value_counts()).reset_index().head(10)\n",
        "    ax=sns.barplot(data=top_frauds(col),x=col,y='count',palette='bone')\n",
        "    ax.bar_label(ax.containers[0])\n",
        "    plt.title(f'Top 10 Frauds | {col}',fontsize=16,fontweight='bold')\n",
        "    plt.xticks(rotation=45,fontweight='bold')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gmAv2NITuo6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(17,15))\n",
        "for idx,val in enumerate(['cc_num','merchant','category','city','job','age']):\n",
        "    plt.subplot(3,2,idx+1)\n",
        "    bar_plot(val)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8tM3pZPTuo6"
      },
      "outputs": [],
      "source": [
        "\n",
        "sns.catplot(data=df,x='amt',col='is_fraud',kind='box',sharex=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fZ7iOLcTuo7"
      },
      "outputs": [],
      "source": [
        "def pie_bar_plot(col):\n",
        "    print(df[col].value_counts())\n",
        "    sns.set_palette('viridis')\n",
        "    fig,axs=plt.subplots(1,2)\n",
        "    axs[0].pie(df[col].value_counts().values.tolist(),autopct='%.2f%%',textprops={'fontsize':25},explode=[0,0.05],shadow=True)\n",
        "    sns.countplot(data=df,x=col,ax=axs[1])\n",
        "    fig.legend(labels=df[col].value_counts().index.tolist(),loc='upper left',fontsize=20)\n",
        "    fig.tight_layout()\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRaAlWFATuo7"
      },
      "outputs": [],
      "source": [
        "#Gender\n",
        "pie_bar_plot('gender')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4yn_x4sTuo7"
      },
      "outputs": [],
      "source": [
        "#Frauds\n",
        "pie_bar_plot('is_fraud')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYGeFjGbTuo7"
      },
      "outputs": [],
      "source": [
        "#We discover that is_fraud column is imbalanced.\n",
        "#So will fix that later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1XiLSFvTuo7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#What is the most month|day|hour frauds occur?\n",
        "fig,axs = plt.subplots(3,2)\n",
        "#Month\n",
        "df.loc[df['is_fraud']==1,'month'].value_counts().sort_index().plot(kind='line',ax=axs[0,0],marker='o',fontsize=15)\n",
        "axs[0,0].set_xticks(range(0,12))\n",
        "df.loc[df['is_fraud']==1,'month'].value_counts(ascending=True).plot(kind='bar',ax=axs[0,1],fontsize=15)\n",
        "fig.suptitle('Fraudulent Analysis', fontsize=18, fontweight='bold')\n",
        "##Day\n",
        "df.loc[df['is_fraud']==1,'day'].value_counts(ascending=True).plot(kind='line',ax=axs[1,0],marker='o',fontsize=15)\n",
        "df.loc[df['is_fraud']==1,'day'].value_counts(ascending=True).plot(kind='bar',ax=axs[1,1],fontsize=15)\n",
        "#Hour\n",
        "df.loc[df['is_fraud']==1,'hour'].value_counts().sort_index().plot(kind='line',ax=axs[2,0],marker='o',fontsize=15)\n",
        "axs[2,0].set_xticks(range(0,24))\n",
        "df.loc[df['is_fraud']==1,'hour'].value_counts(ascending=True).plot(kind='bar',ax=axs[2,1],fontsize=15)\n",
        "fig.suptitle('Fraudulent Analysis', fontsize=20, fontweight='bold')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwA4zsvlTuo7"
      },
      "source": [
        "#### We conclude that most fraud transactions occurs:\n",
        "- On March\n",
        "- On Sunday\n",
        "- At 10 PM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_pWWlC7Tuo_"
      },
      "outputs": [],
      "source": [
        "df.loc[df['is_fraud']==1,['gender']].value_counts()\n",
        "#Males and females exposed to fraud equally (approximately)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbqKKc0lTuo_"
      },
      "outputs": [],
      "source": [
        "sns.barplot(data=df,x='is_fraud', y='city_pop', ci=None)\n",
        "plt.title('Average city_population for Fraud and Non-Fraud Cases',fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "fraud_population = df[df['is_fraud'] == 1]['city_pop']\n",
        "non_fraud_population = df[df['is_fraud'] == 0]['city_pop']\n",
        "t_stat, p_value = ttest_ind(fraud_population, non_fraud_population)\n",
        "print(f'T-test: t-statistic = {round(t_stat,3)}, p-value = {round(p_value,3)}, p-value<0.05? {p_value<0.05}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvpT5dVdTuo_"
      },
      "outputs": [],
      "source": [
        "#Since we accept the null hypothesis,we conclude that there is no significant difference between means,\n",
        "#We conclude also that city_population does not help us on the target(is_fraud), so we will drop it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAWW0FQ_Tuo_"
      },
      "source": [
        "# Further Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYl73MMkTuo_"
      },
      "outputs": [],
      "source": [
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqRHYpJnTuo_"
      },
      "outputs": [],
      "source": [
        "#Convert gender to binary classification\n",
        "df = pd.get_dummies(df,columns=['gender'],drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTddqqiDTuo_"
      },
      "outputs": [],
      "source": [
        "#We will get the time between transactions for each card\n",
        "#Time=0 for every first transaction and time will be represented in hours.\n",
        "df.sort_values(['cc_num', 'trans_date_trans_time'],inplace=True)\n",
        "df['hours_diff_bet_trans']=((df.groupby('cc_num')[['trans_date_trans_time']].diff())/np.timedelta64(1,'h'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO-PKnTTTuo_"
      },
      "outputs": [],
      "source": [
        "df.loc[df['hours_diff_bet_trans'].isna(),'hours_diff_bet_trans'] = 0\n",
        "df['hours_diff_bet_trans'] = df['hours_diff_bet_trans'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMFp3JPqTupA"
      },
      "outputs": [],
      "source": [
        "# x_0=df.groupby('is_fraud')['hours_diff_bet_trans'].mean().values[0]\n",
        "# x_1=df.groupby('is_fraud')['hours_diff_bet_trans'].mean().values[1]\n",
        "# std_0=df.groupby('is_fraud')['hours_diff_bet_trans'].std().values[0]\n",
        "# std_1=df.groupby('is_fraud')['hours_diff_bet_trans'].std().values[1]\n",
        "# n_0=df.groupby('is_fraud')['hours_diff_bet_trans'].count().values[0]\n",
        "# n_1=df.groupby('is_fraud')['hours_diff_bet_trans'].count().values[1]\n",
        "# numerator = x_0 - x_1\n",
        "# domin = np.sqrt(std_0**2/n_0 + std_1**2/n_1)\n",
        "# t_stat=numerator/domin\n",
        "# p_val = 2*(1-t.cdf(abs(t_stat),df=n_0+n_1-2))\n",
        "# print(t_stat,p_val)\n",
        "#----------------------\n",
        "#The power of pingouin library!\n",
        "print(pingouin.ttest(df[df['is_fraud'] == 0]['hours_diff_bet_trans'],\n",
        "              df[df['is_fraud'] == 1]['hours_diff_bet_trans'],\n",
        "              alternative='two-sided')[['T','p-val']])\n",
        "sns.barplot(data=df,x='is_fraud',y='hours_diff_bet_trans',ci=None)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bIYCYrTTupA"
      },
      "outputs": [],
      "source": [
        "#Since p-val < 0.05,we reject the null hypothesis.\n",
        "#The mean of hours is significantly different between frauds and non-frauds transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta1qRC0-TupA"
      },
      "outputs": [],
      "source": [
        "#Make day feature numerical\n",
        "df['day'] = df['trans_date_trans_time'].dt.weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV2mxEW-TupA"
      },
      "outputs": [],
      "source": [
        "#Handling and extracting features from cc_num\n",
        "freq = df.groupby('cc_num').size()\n",
        "df['cc_freq'] = df['cc_num'].apply(lambda x : freq[x])\n",
        "df[['cc_num','cc_freq']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wBhZkaXTupA"
      },
      "outputs": [],
      "source": [
        "#We got freq for each cc_num\n",
        "def hist_show(col):\n",
        "    fig,axs = plt.subplots(1,2,sharex=True)\n",
        "    for i in [0,1]:\n",
        "        sns.histplot(df[df[\"is_fraud\"]==i][col], bins=6,ax=axs[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEBgpZrWTupA"
      },
      "outputs": [],
      "source": [
        "hist_show('cc_freq')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sal5ydTjTupA"
      },
      "outputs": [],
      "source": [
        "def class_det(x):\n",
        "    for idx,val in enumerate(list(range(800,5000,800))):\n",
        "        if x < val:\n",
        "            return idx+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPfFSaM7TupA"
      },
      "outputs": [],
      "source": [
        "df['cc_freq_class'] = df['cc_freq'].apply(class_det)\n",
        "print(df['cc_freq_class'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd0lGWifTupA"
      },
      "outputs": [],
      "source": [
        "hist_show('cc_freq_class')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijjbPmITTupA"
      },
      "source": [
        "#### Now clearly frauds occurs more in credit cards with less use (new ones) and for genuine transactions, it follows a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX08_kHHTupA"
      },
      "outputs": [],
      "source": [
        "#Drop unecessary columns\n",
        "df.drop(columns=['cc_num','trans_date_trans_time','city_pop'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3GG2mJETupA"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q5NNcORTupB"
      },
      "outputs": [],
      "source": [
        "#Reorder columns\n",
        "df = df[['cc_freq','cc_freq_class','city','job','age','gender_M','merchant', 'category',\n",
        "         'distance_km','month','day','hour','hours_diff_bet_trans','amt','is_fraud','split']]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WnU1krtTupB"
      },
      "outputs": [],
      "source": [
        "#We will encode ('city','job','merchant', 'category') preparing for our model using WOE encoder\n",
        "for col in ['city','job','merchant', 'category']:\n",
        "    df[col] = WOEEncoder().fit_transform(df[col],df['is_fraud'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmMPSc1hTupB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# WOE > 0: The category is more likely associated with (fraud)\n",
        "# WOE < 0: The category is more likely associated with (non-fraud)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K87JTpATupB"
      },
      "outputs": [],
      "source": [
        "x_train = df[df['split']=='train'].drop(['split','is_fraud'],axis=1)\n",
        "y_train = df[df['split']=='train']['is_fraud']\n",
        "x_test = df[df['split']=='test'].drop(['split','is_fraud'],axis=1)\n",
        "y_test = df[df['split']=='test']['is_fraud']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK48DDgUTupB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(3,3))\n",
        "plt.pie([len(x_train),len(x_test)],autopct='%.2f%%'\n",
        "        ,textprops={'color':'white'},explode=[0,0.05],shadow=True)\n",
        "plt.legend(['Train','Test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWe4Ug2WTupB"
      },
      "outputs": [],
      "source": [
        "#Handling outliers\n",
        "x_train.select_dtypes(include='number').columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE5bcHpfTupB"
      },
      "outputs": [],
      "source": [
        "datasets={'x_train':x_train,'x_test':x_test}\n",
        "cols = ['hours_diff_bet_trans', 'amt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4aXb7o0TupB"
      },
      "outputs": [],
      "source": [
        "#Outliers in train data before scaling\n",
        "def count_outliers(l:list):\n",
        "    for col in l:\n",
        "        print(f'Outliers In {col}:',len(detect_outliers(x_train,0,[col])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EWPg-9wTupB"
      },
      "outputs": [],
      "source": [
        "def boxplot_outliers(ds:dict):\n",
        "    plt.figure(figsize=(18,8))\n",
        "    c=1\n",
        "    for _,df_x in ds.items():\n",
        "        for col in ['hours_diff_bet_trans', 'amt']:\n",
        "            plt.subplot(2,2,c)\n",
        "            df_x[col].plot(kind='box',vert=False)\n",
        "            c+=1\n",
        "    plt.suptitle('Detecting Outliers In Train|Test Data',fontsize=20)\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwQDxAE5TupB"
      },
      "outputs": [],
      "source": [
        "count_outliers(cols)\n",
        "boxplot_outliers(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTlZ33NhTupB"
      },
      "outputs": [],
      "source": [
        "#Applying log scale\n",
        "for col in cols:\n",
        "    x_train[col] = np.log1p(x_train[col])\n",
        "    x_test[col] = np.log1p(x_test[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulyDdQVZTupC"
      },
      "outputs": [],
      "source": [
        "count_outliers(cols)\n",
        "boxplot_outliers(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV8Ob47aTupC"
      },
      "source": [
        "#### As we can see the log transformation make the distribution more symmetric and reduce the impact of extreme values(We did not removed them).Also we can deal with these data separately,But for now: log functions are commonly used to handle right-skewed distributions. After applying our model we can revert data with exp. function to recover the actual data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDoPKdHzTupC"
      },
      "outputs": [],
      "source": [
        "#Note:-\n",
        "#Logistic Regression often benefits from feature scaling.Since the algorithm uses the weights assigned to features during\n",
        "#training, and having features on similar scales can help the optimization process converge faster.\n",
        "#While Decision Trees, including Random Forest (an ensemble of decision trees), are generally not sensitive to the scale of the\n",
        "#features.They make decisions based on splitting criteria and don't rely on the absolute values of the features.\n",
        "scaler = StandardScaler().fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trrwW6w6TupC"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsY8K1-lTupC"
      },
      "source": [
        "#### We want to avoid false negatives as much as possible.A false negative case means that a fraud-positive transaction is assessed to genuine transaction,In this use case false positives (a genuine transaction as fraud-positive) are not as important as preventing a fraud. So our focus is about (RECALL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNdtbTD1TupC"
      },
      "outputs": [],
      "source": [
        "#Let's try our model without handling the imbalance data of fraud feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODA4ZQvRTupC"
      },
      "outputs": [],
      "source": [
        "evl_models = {'Logistic Regression':LogisticRegression(random_state=10),\n",
        "          'Decision Tree':DecisionTreeClassifier(random_state=10),\n",
        "          'Random Forest':RandomForestClassifier(random_state=10)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE7FXBGeTupC"
      },
      "outputs": [],
      "source": [
        "def evl_model(xtrain,ytrain,xtest,ytest):\n",
        "    sns.set(rc={'figure.figsize':(18,6)})\n",
        "    i=0\n",
        "    for name,model in evl_models.items():\n",
        "        fig,axs=plt.subplots(1,2)\n",
        "        print('Model : '+name)\n",
        "        print('_'*30)\n",
        "        start=time()\n",
        "        model.fit(xtrain,ytrain)\n",
        "        y_pred = model.predict(xtest)\n",
        "        end=time()\n",
        "        #Confusion Matrix\n",
        "        cm = confusion_matrix(ytest,y_pred,labels=model.classes_)\n",
        "        cmd = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n",
        "        cmd.plot(colorbar=False,ax=axs[i])\n",
        "        axs[i].grid(False)\n",
        "        #AUC-ROC Curve\n",
        "        y_proba = model.predict_proba(xtest)\n",
        "        fpr, tpr, _ = roc_curve(ytest, y_proba[:, 1])\n",
        "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=axs[i+1])\n",
        "        axs[i+1].plot([0, 1], [0, 1], color = 'g')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        #We focus on recall=TP/TP+FN\n",
        "        #such that it's the score that model predict(non-fraud) while it's fraud.\n",
        "        print('Recall Score: ',recall_score(ytest,y_pred))\n",
        "        print('Precision: ',precision_score(ytest,y_pred))\n",
        "        print('F1-Score: ',f1_score(ytest,y_pred))\n",
        "        print('Accuracy Score: ',accuracy_score(ytest,y_pred))\n",
        "        print('AUC Score: ',roc_auc_score(ytest,y_proba[:,1]))\n",
        "        print('Running Time : ',round((end-start)/60.0,2),'Mins')\n",
        "        print('*'*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3KDqR-LTupC",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "evl_model(x_train,y_train,x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM4flHbVTupC"
      },
      "source": [
        "**Conclusions (From Imbalanced Data)**\n",
        "- Logistic Regression : Gives a very weak recall accuracy\n",
        "- Decision Tree : Gives the highest recall accuracy but has a low precision\n",
        "- Random Forest : Gives high recall accuracy and high precision\n",
        "- Note: Our focus is not on the total model accuracy (Confusion-Matrix Accuracy) TP+TN/TP+TN+FP+FN,Since we focus on the best accuracy for the model to predict the real fraud transaction and don't predict non-fraud and we detect it from the (True Positive Rate 'Recall')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSOPMFgqTupD"
      },
      "outputs": [],
      "source": [
        "#Let's apply SMOTE over sampling to make balance between fraud and non-fraud data and see if there is a significant different\n",
        "#Between the accuracies or not\n",
        "smote = SMOTE()\n",
        "x_train,y_train = smote.fit_resample(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJHK4UfrTupD"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts().plot(kind='pie',figsize=(4,4),autopct='%.2f%%',textprops={'color':'white'},labels=['Fraud','Non-Fraud'],legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1COJxcTTupD"
      },
      "outputs": [],
      "source": [
        "evl_model(x_train,y_train,x_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViIhE2UZTupD"
      },
      "source": [
        "## Model Evaluation Considerations\n",
        "\n",
        "### Recall (Sensitivity):\n",
        "\n",
        "- High Recall is crucial in fraud detection to minimize the number of false negatives (missed fraud cases).\n",
        "- A higher Recall indicates better sensitivity to detecting fraudulent transactions.\n",
        "\n",
        "### Precision:\n",
        "\n",
        "- Precision is important to minimize false positives.\n",
        "- In fraud detection, a false positive might lead to inconveniencing legitimate customers.\n",
        "- Balancing precision and recall is essential.\n",
        "\n",
        "### F1-Score:\n",
        "\n",
        "- F1-Score provides a balance between precision and recall.\n",
        "- A higher F1-Score indicates a better balance between minimizing false positives and false negatives.\n",
        "\n",
        "### Accuracy:\n",
        "\n",
        "- While high accuracy is generally desirable, in imbalanced datasets (where fraud cases are rare), it might not be the most informative metric.\n",
        "- A model could achieve high accuracy by predicting the majority class.\n",
        "\n",
        "### AUC Score:\n",
        "\n",
        "- AUC (Area Under the ROC Curve) provides a summary measure across various classification thresholds.\n",
        "- A higher AUC indicates better overall model performance.\n",
        "\n",
        "## Recommendation\n",
        "\n",
        "- **RandomForest** appears to perform well across multiple metrics regardless it takes the higher time complexity, it provides a good balance between recall, precision, and accuracy.\n",
        "- **DecisionTree** appears to perform well across recall , if we consider to minimize the number of false negatives (missed fraud cases) it will be the best solution, but it fails to make a good predictions on non-fraud transactions that might lead to inconveniencing legitimate customers\n",
        "- **LogisticRegression** appears to perform well across recall, but gives a bad precision and accuracy so it's out of scope for our project"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Credit Card Fraud Detection",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 817870,
          "sourceId": 1399887,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
